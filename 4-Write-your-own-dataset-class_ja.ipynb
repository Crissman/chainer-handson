{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# データセットクラスを書いてみよう\n",
    "\n",
    "ここでは、Chainerにすでに用意されているCIFAR10のデータを取得する機能を使って、データセットクラスを自分で書いてみます。Chainerでは、データセットを表すクラスは以下の機能を持っていることが必要とされます。\n",
    "\n",
    "- データセット内のデータ数を返す`__len__`メソッド\n",
    "- 引数として渡される`i`に対応したデータもしくはデータとラベルの組を返す`get_example`メソッド\n",
    "\n",
    "その他のデータセットに必要な機能は、`chainer.dataset.DatasetMixin`クラスを継承することで用意できます。ここでは、`DatasetMixin`クラスを継承し、Data augmentation機能のついたデータセットクラスを作成してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. CIFAR10データセットクラスを書く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chainer import dataset\n",
    "from chainer.datasets import cifar\n",
    "\n",
    "class CIFAR10Augmented(dataset.DatasetMixin):\n",
    "\n",
    "    def __init__(self, train=True):\n",
    "        train_data, test_data = cifar.get_cifar10()\n",
    "        if train:\n",
    "            self.data = train_data\n",
    "        else:\n",
    "            self.data = test_data\n",
    "        self.train = train\n",
    "        self.random_crop = 4\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        x, t = self.data[i]\n",
    "        if self.train:\n",
    "            x = x.transpose(1, 2, 0)\n",
    "            h, w, _ = x.shape\n",
    "            x_offset = np.random.randint(self.random_crop)\n",
    "            y_offset = np.random.randint(self.random_crop)\n",
    "            x = x[y_offset:y_offset + h - self.random_crop,\n",
    "                  x_offset:x_offset + w - self.random_crop]\n",
    "            if np.random.rand() > 0.5:\n",
    "                x = np.fliplr(x)\n",
    "            x = x.transpose(2, 0, 1)\n",
    "        return x, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "このクラスは、CIFAR10のデータのそれぞれに対し、\n",
    "\n",
    "- 32x32の大きさの中からランダムに28x28の領域をクロップ\n",
    "- 1/2の確率で左右を反転させる\n",
    "\n",
    "という加工を行っています。こういった操作を加えることで擬似的に学習データのバリエーションを増やすと、オーバーフィッティングを抑制することに役に立つということが知られています。これらの操作以外にも、画像の色味を変化させるような変換やランダムな回転、アフィン変換など、さまざまな加工によって学習データ数を擬似的に増やす方法が提案されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. 作成したデータセットクラスを使って学習を行う\n",
    "\n",
    "それではさっそくこの`CIFAR10`クラスを使って学習を行ってみましょう。先程使ったのと同じ大きなネットワークを使うことで、Data augmentationの効果がどの程度あるのかを調べてみましょう。`train`関数も含め、データセットクラス以外は先程使用したコードと同じになっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   main/accuracy  validation/main/loss  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           1.93723     0.285566       1.61129               0.408539                  18.5154       \n",
      "\u001b[J2           1.42688     0.482494       1.30301               0.542297                  36.693        \n",
      "\u001b[J3           1.17734     0.591849       1.05394               0.633758                  55.0077       \n",
      "\u001b[J4           1.02439     0.648928       0.947258              0.683718                  73.3849       \n",
      "\u001b[J5           0.91796     0.696791       0.858783              0.714172                  91.8993       \n",
      "\u001b[J6           0.814746    0.734835       0.810849              0.740446                  110.435       \n",
      "\u001b[J7           0.752699    0.757883       0.778417              0.750498                  128.974       \n",
      "\u001b[J8           0.692943    0.774988       0.732471              0.765525                  147.552       \n",
      "\u001b[J9           0.644897    0.790741       0.668969              0.781449                  166.234       \n",
      "\u001b[J10          0.601728    0.803357       0.633086              0.799164                  184.913       \n",
      "\u001b[J11          0.588226    0.8117         0.606026              0.806031                  203.602       \n",
      "\u001b[J12          0.548037    0.822563       0.586106              0.809614                  222.262       \n",
      "\u001b[J13          0.517098    0.830862       0.666632              0.805235                  240.945       \n",
      "\u001b[J14          0.538039    0.825704       0.662914              0.794586                  259.601       \n",
      "\u001b[J15          0.496069    0.84329        0.527605              0.833997                  278.308       \n",
      "\u001b[J16          0.480762    0.847291       0.603582              0.827727                  296.86        \n",
      "\u001b[J17          0.444903    0.858456       0.566464              0.83121                   315.428       \n",
      "\u001b[J18          0.42393     0.862796       0.526133              0.839371                  334.008       \n",
      "\u001b[J19          0.402406    0.872279       0.568653              0.838973                  352.542       \n",
      "\u001b[J20          0.408313    0.867958       0.500856              0.842357                  371.073       \n",
      "\u001b[J21          0.379207    0.878157       0.485137              0.846736                  389.641       \n",
      "\u001b[J22          0.364486    0.882903       0.509811              0.84594                   408.151       \n",
      "\u001b[J23          0.351996    0.881942       0.496596              0.853205                  426.647       \n",
      "\u001b[J24          0.384981    0.87498        0.514132              0.846537                  445.131       \n",
      "\u001b[J25          0.345489    0.887468       0.489725              0.855096                  463.694       \n",
      "\u001b[J26          0.32875     0.890865       0.548287              0.837878                  482.19        \n",
      "\u001b[J27          0.333036    0.890885       0.490093              0.857086                  500.7         \n",
      "\u001b[J28          0.308984    0.897007       0.469772              0.858081                  519.234       \n",
      "\u001b[J29          0.368327    0.882573       0.510205              0.843451                  537.762       \n",
      "\u001b[J30          0.301466    0.902169       0.486552              0.864252                  556.269       \n",
      "\u001b[J31          0.291633    0.90723        0.448381              0.868133                  574.796       \n",
      "\u001b[J32          0.268337    0.916333       0.483037              0.867635                  593.304       \n",
      "\u001b[J33          0.255648    0.9168         0.477294              0.867138                  611.833       \n",
      "\u001b[J34          0.249392    0.919234       0.469037              0.873109                  630.332       \n",
      "\u001b[J35          0.235389    0.922195       0.472681              0.872014                  648.868       \n",
      "\u001b[J36          0.233415    0.923656       0.458594              0.869128                  667.355       \n",
      "\u001b[J37          0.221172    0.929088       0.463465              0.869526                  685.856       \n",
      "\u001b[J38          0.214483    0.928737       0.441722              0.879578                  704.388       \n",
      "\u001b[J39          0.216198    0.931038       0.470847              0.874104                  722.871       \n",
      "\u001b[J40          0.308726    0.909431       0.472775              0.870322                  741.345       \n",
      "\u001b[J41          0.206207    0.932325       0.462302              0.882663                  759.869       \n",
      "\u001b[J42          0.192034    0.93726        0.456685              0.881668                  778.439       \n",
      "\u001b[J43          0.179062    0.941001       0.462844              0.880175                  796.93        \n",
      "\u001b[J44          0.225368    0.928177       0.494072              0.871318                  815.402       \n",
      "\u001b[J45          0.192742    0.936941       0.474624              0.881369                  833.942       \n",
      "\u001b[J46          0.180091    0.940421       0.468161              0.884554                  852.404       \n",
      "\u001b[J47          0.180724    0.939641       0.516513              0.878782                  870.864       \n",
      "\u001b[J48          0.178194    0.941201       0.437027              0.885848                  889.371       \n",
      "\u001b[J49          0.176283    0.942795       0.535836              0.874005                  907.925       \n",
      "\u001b[J50          0.173888    0.941461       0.480066              0.88336                   926.465       \n",
      "\u001b[J51          0.169948    0.943762       0.474535              0.876294                  944.956       \n",
      "\u001b[J52          0.167653    0.945723       0.500818              0.884256                  963.431       \n",
      "\u001b[J53          0.163582    0.946571       0.494321              0.883459                  981.963       \n",
      "\u001b[J54          0.158371    0.948604       0.496801              0.882166                  1000.42       \n",
      "\u001b[J55          0.152969    0.950504       0.474192              0.883559                  1018.91       \n",
      "\u001b[J56          0.156136    0.949504       0.539529              0.884952                  1037.34       \n",
      "\u001b[J57          0.148764    0.952605       0.546312              0.882365                  1055.9        \n",
      "\u001b[J58          0.149318    0.952485       0.468698              0.887042                  1074.47       \n",
      "\u001b[J59          0.145386    0.953085       0.475614              0.887838                  1092.98       \n",
      "\u001b[J60          0.14057     0.954165       0.569231              0.883559                  1111.48       \n",
      "\u001b[J61          0.14409     0.953185       0.511002              0.882862                  1130.05       \n",
      "\u001b[J62          0.138028    0.955006       0.619937              0.883061                  1148.59       \n",
      "\u001b[J63          0.135399    0.956446       0.555571              0.884355                  1167.11       \n",
      "\u001b[J64          0.133266    0.956686       0.483841              0.890227                  1185.63       \n",
      "\u001b[J65          0.13233     0.957541       0.648737              0.879678                  1204.16       \n",
      "\u001b[J66          0.13708     0.955706       0.553954              0.888435                  1222.61       \n",
      "\u001b[J67          0.126614    0.960347       0.530032              0.884156                  1241.12       \n",
      "\u001b[J68          0.125786    0.960527       0.605024              0.884057                  1259.68       \n",
      "\u001b[J69          0.121641    0.961557       0.566451              0.883658                  1278.22       \n",
      "\u001b[J70          0.289221    0.932478       0.624388              0.861565                  1296.74       \n",
      "\u001b[J71          0.187206    0.939561       0.590414              0.877488                  1315.32       \n",
      "\u001b[J72          0.143774    0.953965       0.546171              0.887341                  1333.85       \n",
      "\u001b[J73          0.119004    0.961117       0.586789              0.882166                  1352.4        \n",
      "\u001b[J74          0.115446    0.963208       0.579778              0.88963                   1370.92       \n",
      "\u001b[J75          0.109808    0.964969       0.556509              0.886047                  1389.42       \n",
      "\u001b[J76          0.104628    0.966749       0.594297              0.879479                  1407.86       \n",
      "\u001b[J77          0.104402    0.966512       0.648005              0.882464                  1426.31       \n",
      "\u001b[J78          0.106217    0.965809       0.615655              0.886146                  1444.79       \n",
      "\u001b[J79          0.108468    0.966469       0.583344              0.883161                  1463.23       \n",
      "\u001b[J80          0.102108    0.966809       0.609545              0.892416                  1481.67       \n",
      "\u001b[J81          0.107968    0.964874       0.612285              0.89172                   1500.26       \n",
      "\u001b[J82          0.102005    0.966989       0.522442              0.893511                  1518.79       \n",
      "\u001b[J83          0.099499    0.96825        0.634319              0.88744                   1537.32       \n",
      "\u001b[J84          0.106363    0.966149       0.612157              0.887639                  1555.75       \n",
      "\u001b[J85          0.111244    0.964974       0.547136              0.887639                  1574.23       \n",
      "\u001b[J86          0.104024    0.96741        0.540066              0.893611                  1592.65       \n",
      "\u001b[J87          0.10375     0.96721        0.543003              0.881469                  1611.04       \n",
      "\u001b[J88          0.104028    0.96745        0.553459              0.89162                   1629.48       \n",
      "\u001b[J89          0.0993184   0.96849        0.63708               0.890127                  1647.98       \n",
      "\u001b[J90          0.104134    0.96791        0.602768              0.884554                  1666.48       \n",
      "\u001b[J91          0.104915    0.96717        0.621538              0.889132                  1685.02       \n",
      "\u001b[J92          0.100241    0.96933        0.591726              0.888435                  1703.53       \n",
      "\u001b[J93          0.0994485   0.96885        0.552689              0.887241                  1722.05       \n",
      "\u001b[J94          0.0946836   0.96899        0.589193              0.890824                  1740.56       \n",
      "\u001b[J95          0.0919512   0.970491       0.790633              0.887938                  1759.09       \n",
      "\u001b[J96          0.0963245   0.96961        0.748226              0.887739                  1777.57       \n",
      "\u001b[J97          0.0899946   0.971308       0.659836              0.888137                  1796.11       \n",
      "\u001b[J98          0.0969342   0.96945        0.70236               0.890227                  1814.67       \n",
      "\u001b[J99          0.0906312   0.971571       0.623863              0.891123                  1833.18       \n",
      "\u001b[J100         0.0908828   0.972011       0.684595              0.893909                  1851.7        \n"
     ]
    }
   ],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.datasets import cifar\n",
    "from chainer import iterators\n",
    "from chainer import optimizers\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "\n",
    "class ConvBlock(chainer.Chain):\n",
    "    \n",
    "    def __init__(self, n_ch, pool_drop=False):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        super(ConvBlock, self).__init__(\n",
    "            conv=L.Convolution2D(None, n_ch, 3, 1, 1,\n",
    "                                 nobias=True, initialW=w),\n",
    "            bn=L.BatchNormalization(n_ch)\n",
    "        )\n",
    "        \n",
    "        self.train = True\n",
    "        self.pool_drop = pool_drop\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.bn(self.conv(x)))\n",
    "        if self.pool_drop:\n",
    "            h = F.max_pooling_2d(h, 2, 2)\n",
    "            h = F.dropout(h, ratio=0.25, train=self.train)\n",
    "        return h\n",
    "    \n",
    "class LinearBlock(chainer.Chain):\n",
    "    \n",
    "    def __init__(self):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        super(LinearBlock, self).__init__(\n",
    "            fc=L.Linear(None, 1024, initialW=w))\n",
    "        self.train = True\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return F.dropout(F.relu(self.fc(x)), ratio=0.5, train=self.train)\n",
    "    \n",
    "class DeepCNN(chainer.ChainList):\n",
    "\n",
    "    def __init__(self, n_output):\n",
    "        super(DeepCNN, self).__init__(\n",
    "            ConvBlock(64),\n",
    "            ConvBlock(64, True),\n",
    "            ConvBlock(128),\n",
    "            ConvBlock(128, True),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256, True),\n",
    "            LinearBlock(),\n",
    "            LinearBlock(),\n",
    "            L.Linear(None, n_output)\n",
    "        )\n",
    "        self._train = True\n",
    "            \n",
    "    @property\n",
    "    def train(self):\n",
    "        return self._train\n",
    "            \n",
    "    @train.setter\n",
    "    def train(self, val):\n",
    "        self._train = val\n",
    "        for c in self.children():\n",
    "            c.train = val\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for f in self.children():\n",
    "            x = f(x)\n",
    "        return x\n",
    "\n",
    "def train(model_object, batchsize=64, gpu_id=0, max_epoch=100):\n",
    "\n",
    "    # 1. Dataset\n",
    "    train, test = CIFAR10Augmented(), CIFAR10Augmented(False)\n",
    "\n",
    "    # 2. Iterator\n",
    "    train_iter = iterators.SerialIterator(train, batchsize)\n",
    "    test_iter = iterators.SerialIterator(test, batchsize, False, False)\n",
    "\n",
    "    # 3. Model\n",
    "    model = L.Classifier(model_object)\n",
    "    model.to_gpu(gpu_id)\n",
    "\n",
    "    # 4. Optimizer\n",
    "    optimizer = optimizers.Adam()\n",
    "    optimizer.setup(model)\n",
    "\n",
    "    # 5. Updater\n",
    "    updater = training.StandardUpdater(train_iter, optimizer, device=gpu_id)\n",
    "\n",
    "    # 6. Trainer\n",
    "    trainer = training.Trainer(updater, (max_epoch, 'epoch'), out='{}_cifar10_result'.format(model_object.__class__.__name__))\n",
    "\n",
    "    # 7. Evaluator\n",
    "\n",
    "    class TestModeEvaluator(extensions.Evaluator):\n",
    "\n",
    "        def evaluate(self):\n",
    "            model = self.get_target('main')\n",
    "            model.train = False\n",
    "            ret = super(TestModeEvaluator, self).evaluate()\n",
    "            model.train = True\n",
    "            return ret\n",
    "\n",
    "    trainer.extend(extensions.LogReport())\n",
    "    trainer.extend(TestModeEvaluator(test_iter, model, device=gpu_id))\n",
    "    trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'main/accuracy', 'validation/main/loss', 'validation/main/accuracy', 'elapsed_time']))\n",
    "    trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss'], x_key='epoch', file_name='loss.png'))\n",
    "    trainer.extend(extensions.PlotReport(['main/accuracy', 'validation/main/accuracy'], x_key='epoch', file_name='accuracy.png'))\n",
    "    trainer.run()\n",
    "    del trainer\n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = train(DeepCNN(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
