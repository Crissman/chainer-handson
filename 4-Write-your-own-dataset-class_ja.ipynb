{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# データセットクラスを書いてみよう\n",
    "\n",
    "ここでは、Chainerにすでに用意されているCIFAR10のデータを取得する機能を使って、データセットクラスを自分で書いてみます。Chainerでは、データセットを表すクラスは以下の機能を持っていることが必要とされます。\n",
    "\n",
    "- データセット内のデータ数を返す`__len__`メソッド\n",
    "- 引数として渡される`i`に対応したデータもしくはデータとラベルの組を返す`get_example`メソッド\n",
    "\n",
    "その他のデータセットに必要な機能は、`chainer.dataset.DatasetMixin`クラスを継承することで用意できます。ここでは、`DatasetMixin`クラスを継承し、Data augmentation機能のついたデータセットクラスを作成してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. CIFAR10データセットクラスを書く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chainer import dataset\n",
    "from chainer.datasets import cifar\n",
    "\n",
    "gpu_id = 0  # Set to -1 if you don't have a GPU\n",
    "\n",
    "class CIFAR10Augmented(dataset.DatasetMixin):\n",
    "\n",
    "    def __init__(self, train=True):\n",
    "        train_data, test_data = cifar.get_cifar10()\n",
    "        if train:\n",
    "            self.data = train_data\n",
    "        else:\n",
    "            self.data = test_data\n",
    "        self.train = train\n",
    "        self.random_crop = 4\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        x, t = self.data[i]\n",
    "        if self.train:\n",
    "            x = x.transpose(1, 2, 0)\n",
    "            h, w, _ = x.shape\n",
    "            x_offset = np.random.randint(self.random_crop)\n",
    "            y_offset = np.random.randint(self.random_crop)\n",
    "            x = x[y_offset:y_offset + h - self.random_crop,\n",
    "                  x_offset:x_offset + w - self.random_crop]\n",
    "            if np.random.rand() > 0.5:\n",
    "                x = np.fliplr(x)\n",
    "            x = x.transpose(2, 0, 1)\n",
    "        return x, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "このクラスは、CIFAR10のデータのそれぞれに対し、\n",
    "\n",
    "- 32x32の大きさの中からランダムに28x28の領域をクロップ\n",
    "- 1/2の確率で左右を反転させる\n",
    "\n",
    "という加工を行っています。こういった操作を加えることで擬似的に学習データのバリエーションを増やすと、オーバーフィッティングを抑制することに役に立つということが知られています。これらの操作以外にも、画像の色味を変化させるような変換やランダムな回転、アフィン変換など、さまざまな加工によって学習データ数を擬似的に増やす方法が提案されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. 作成したデータセットクラスを使って学習を行う\n",
    "\n",
    "それではさっそくこの`CIFAR10`クラスを使って学習を行ってみましょう。先程使ったのと同じ大きなネットワークを使うことで、Data augmentationの効果がどの程度あるのかを調べてみましょう。`train`関数も含め、データセットクラス以外は先程使用したコードとほぼ同じになっています。異なるところはエポック数と、保存先ディレクトリ名だけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   main/accuracy  validation/main/loss  validation/main/accuracy  elapsed_time\n"
     ]
    }
   ],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.datasets import cifar\n",
    "from chainer import iterators\n",
    "from chainer import optimizers\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "\n",
    "class ConvBlock(chainer.Chain):\n",
    "    \n",
    "    def __init__(self, n_ch, pool_drop=False):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        super(ConvBlock, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv = L.Convolution2D(None, n_ch, 3, 1, 1,\n",
    "                                 nobias=True, initialW=w)\n",
    "            self.bn = L.BatchNormalization(n_ch)\n",
    "        \n",
    "        \n",
    "        self.pool_drop = pool_drop\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.bn(self.conv(x)))\n",
    "        if self.pool_drop:\n",
    "            h = F.max_pooling_2d(h, 2, 2)\n",
    "            h = F.dropout(h, ratio=0.25)\n",
    "        return h\n",
    "\n",
    "class LinearBlock(chainer.Chain):\n",
    "    \n",
    "    def __init__(self):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        super(LinearBlock, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.fc = L.Linear(None, 1024, initialW=w)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return F.dropout(F.relu(self.fc(x)), ratio=0.5)\n",
    "    \n",
    "class DeepCNN(chainer.ChainList):\n",
    "\n",
    "    def __init__(self, n_output):\n",
    "        super(DeepCNN, self).__init__(\n",
    "            ConvBlock(64),\n",
    "            ConvBlock(64, True),\n",
    "            ConvBlock(128),\n",
    "            ConvBlock(128, True),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256, True),\n",
    "            LinearBlock(),\n",
    "            LinearBlock(),\n",
    "            L.Linear(None, n_output)\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for f in self.children():\n",
    "            x = f(x)\n",
    "        return x\n",
    "\n",
    "def train(model_object, batchsize=64, gpu_id=gpu_id, max_epoch=20):\n",
    "\n",
    "    # 1. Dataset\n",
    "    train, test = CIFAR10Augmented(), CIFAR10Augmented(False)\n",
    "\n",
    "    # 2. Iterator\n",
    "    train_iter = iterators.SerialIterator(train, batchsize)\n",
    "    test_iter = iterators.SerialIterator(test, batchsize, False, False)\n",
    "\n",
    "    # 3. Model\n",
    "    model = L.Classifier(model_object)\n",
    "    if gpu_id >= 0:\n",
    "            model.to_gpu(gpu_id)\n",
    "\n",
    "    # 4. Optimizer\n",
    "    optimizer = optimizers.Adam()\n",
    "    optimizer.setup(model)\n",
    "\n",
    "    # 5. Updater\n",
    "    updater = training.StandardUpdater(train_iter, optimizer, device=gpu_id)\n",
    "\n",
    "    # 6. Trainer\n",
    "    trainer = training.Trainer(updater, (max_epoch, 'epoch'), out='{}_cifar10augmented_result'.format(model_object.__class__.__name__))\n",
    "\n",
    "    # 7. Evaluator\n",
    "\n",
    "    class TestModeEvaluator(extensions.Evaluator):\n",
    "\n",
    "        def evaluate(self):\n",
    "            model = self.get_target('main')\n",
    "            ret = super(TestModeEvaluator, self).evaluate()\n",
    "            return ret\n",
    "\n",
    "    trainer.extend(extensions.LogReport())\n",
    "    trainer.extend(TestModeEvaluator(test_iter, model, device=gpu_id))\n",
    "    trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'main/accuracy', 'validation/main/loss', 'validation/main/accuracy', 'elapsed_time']))\n",
    "    trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss'], x_key='epoch', file_name='loss.png'))\n",
    "    trainer.extend(extensions.PlotReport(['main/accuracy', 'validation/main/accuracy'], x_key='epoch', file_name='accuracy.png'))\n",
    "    trainer.run()\n",
    "    del trainer\n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = train(DeepCNN(10), max_epoch=100, gpu_id=gpu_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "先程のData augmentationなしの場合は87%程度で頭打ちになっていた精度が、学習データにaugmentationを施すことで89%以上まで向上させられることが分かりました。2%強の改善です。\n",
    "\n",
    "最後に、ロスと精度のグラフを見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='DeepCNN_cifar10augmented_result/loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='DeepCNN_cifar10augmented_result/accuracy.png')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
